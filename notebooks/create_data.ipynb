{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noamatia/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from tqdm import tqdm\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth import is_bfloat16_supported\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapenet_core_dir = \"/scratch/noam/ShapeNetCore.v1\"\n",
    "data_csv_path = \"/home/noamatia/repos/PEPSI/data/shapetalk.csv\"\n",
    "finetune_csv_path = \"/home/noamatia/repos/PEPSI/data/finetune_llama3.csv\"\n",
    "shapetalk_csv_path = (\n",
    "    \"/scratch/noam/shapetalk/language/shapetalk_preprocessed_public_version_0.csv\"\n",
    ")\n",
    "shape_category_to_synset_id = {\n",
    "    \"chair\": \"03001627\",\n",
    "    \"table\": \"04379243\",\n",
    "    \"lamp\": \"03636649\",\n",
    "}\n",
    "shape_category_to_part_to_key_words = {\n",
    "    \"chair\": {\n",
    "        \"back\": [\"back\", \"spindle\"],\n",
    "        \"leg\": [\"leg\", \"feet\", \"foot\", \"stretcher\", \"wheel\"],\n",
    "        \"arm\": [\"arm\", \"handle\"],\n",
    "        \"seat\": [\"seat\", \"base\", \"apron\"],\n",
    "    },\n",
    "    \"table\": {\n",
    "        \"top\": [\"top\", \"apron\", \"surface\", \"skirt\"],\n",
    "        \"leg\": [\"leg\", \"feet\", \"foot\", \"wheel\"],\n",
    "        \"support\": [\"support\", \"brace\", \"stretcher\", \"crossbar\"],\n",
    "    },\n",
    "    \"lamp\": {\n",
    "        \"shade\": [\"shade\", \"top\"],\n",
    "        \"base\": [\"base\"],\n",
    "        \"tube\": [\n",
    "            \"tube\",\n",
    "            \"leg\",\n",
    "            \"rod\",\n",
    "            \"arm\",\n",
    "            \"neck\",\n",
    "            \"pole\",\n",
    "            \"strut\",\n",
    "            \"body\",\n",
    "            \"column\",\n",
    "        ],\n",
    "        \"bulb\": [\"bulb\", \"light\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create first version of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(shapetalk_csv_path)\n",
    "df = df[df.source_object_class.isin(list(shape_category_to_synset_id.keys()))]\n",
    "df = df[df.changeit_split.isin([\"train\", \"test\"])]\n",
    "df.to_csv(data_csv_path, index=True, index_label=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add wnlemmas to samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(data_csv_path, index_col=\"id\")\n",
    "df[\"source_wnlemmas\"], df[\"target_wnlemmas\"] = \"\", \"\"\n",
    "df[\"source_random_wnlemma\"], df[\"target_random_wnlemma\"] = \"\", \"\"\n",
    "\n",
    "\n",
    "def build_shapenetuid_to_wnlemmas(synset_id):\n",
    "    data_dict = {}\n",
    "    with open(f\"{shapenet_core_dir}/{synset_id}.csv\", mode=\"r\") as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            data_dict[row[\"fullId\"].removeprefix(\"3dw.\")] = row[\"wnlemmas\"]\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "shape_category_to_shapenetuid_to_wnlemmas = {}\n",
    "for category, synset_id in shape_category_to_synset_id.items():\n",
    "    shape_category_to_shapenetuid_to_wnlemmas[category] = build_shapenetuid_to_wnlemmas(\n",
    "        synset_id\n",
    "    )\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if row.source_dataset == \"ShapeNet\":\n",
    "        wnlemmas = shape_category_to_shapenetuid_to_wnlemmas[row.source_object_class][\n",
    "            row.source_model_name\n",
    "        ]\n",
    "        df.at[i, \"source_wnlemmas\"] = wnlemmas\n",
    "        df.at[i, \"source_random_wnlemma\"] = random.choice(wnlemmas.split(\",\"))\n",
    "    else:\n",
    "        df.at[i, \"source_wnlemmas\"] = row.source_object_class\n",
    "        df.at[i, \"source_random_wnlemma\"] = row.source_object_class\n",
    "    if row.target_dataset == \"ShapeNet\":\n",
    "        wnlemmas = shape_category_to_shapenetuid_to_wnlemmas[row.target_object_class][\n",
    "            row.target_model_name\n",
    "        ]\n",
    "        df.at[i, \"target_wnlemmas\"] = wnlemmas\n",
    "        df.at[i, \"target_random_wnlemma\"] = random.choice(wnlemmas.split(\",\"))\n",
    "    else:\n",
    "        df.at[i, \"target_wnlemmas\"] = row.target_object_class\n",
    "        df.at[i, \"target_random_wnlemma\"] = row.target_object_class\n",
    "\n",
    "df.to_csv(data_csv_path, index=True, index_label=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add part annotation by keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_csv_path, index_col=\"id\")\n",
    "\n",
    "def utterance_and_shape_category_to_part(utterance: str, shape_category: str) -> str:\n",
    "    parts = [part for part, keywords in shape_category_to_part_to_key_words[shape_category].items() if any(keyword in utterance for keyword in keywords)]\n",
    "    if len(parts) == 1:\n",
    "        return parts[0]\n",
    "    else:\n",
    "        return \"none\"\n",
    "    \n",
    "df[\"part_keywords\"] = df.apply(lambda row: utterance_and_shape_category_to_part(row.utterance, row.source_object_class), axis=1)\n",
    "df.to_csv(data_csv_path, index=True, index_label=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama3\n",
    "[source](https://colab.research.google.com/github/A3597466/unsloth-T4/blob/main/Alpaca_%2B_Llama_3_8b_full_example.ipynb#scrollTo=kR3gIAX-SM2q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_csv_path, index_col=\"id\")\n",
    "if os.path.exists(finetune_csv_path):\n",
    "    finetune_df = pd.read_csv(finetune_csv_path, index_col=\"id\")\n",
    "else:\n",
    "    finetune_df = df.sample(300)\n",
    "    finetune_df[\"utterance_llama3\"], finetune_df[\"part\"] = \"\", \"\"\n",
    "    for i, row in tqdm(finetune_df.iterrows(), total=len(finetune_df)):\n",
    "        print(row.target_random_wnlemma, row.utterance)\n",
    "        finetune_df[\"utterance_llama3\"][i] = input(\"Enter the correct utterance: \")\n",
    "        finetune_df[\"part_llama3\"][i] = input(\"Enter the part of the utterance: \")\n",
    "        print()\n",
    "    finetune_df.to_csv(finetune_csv_path, index=True, index_label=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama3Dataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        arrow_table = pa.Table.from_pandas(df)\n",
    "        super().__init__(arrow_table=arrow_table)\n",
    "\n",
    "\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples, eos_token):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        text = alpaca_prompt.format(instruction, input, output) + eos_token\n",
    "        texts.append(text)\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }\n",
    "\n",
    "\n",
    "def build_dataset(df, fn_instruction, fn_input, fn_output, eos_token):\n",
    "    headers, rows = [\"instruction\", \"input\", \"output\"], []\n",
    "    for _, row in df.iterrows():\n",
    "        instruction = fn_instruction(row)\n",
    "        input = fn_input(row)\n",
    "        output = fn_output(row)\n",
    "        rows.append([instruction, input, output])\n",
    "    dataset = Llama3Dataset(pd.DataFrame(rows, columns=headers))\n",
    "    dataset = dataset.map(\n",
    "        lambda examples: formatting_prompts_func(examples, eos_token),\n",
    "        batched=True,\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def build_model_and_tokenizer():\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"unsloth/llama-3-8b-bnb-4bit\",\n",
    "        max_seq_length=2048,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=16,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=3407,\n",
    "        use_rslora=False,\n",
    "        loftq_config=None,\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def build_trainer(model, tokenizer, finetune_dataset):\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=finetune_dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=2048,\n",
    "        dataset_num_proc=2,\n",
    "        packing=False,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=5,\n",
    "            max_steps=60,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=not is_bfloat16_supported(),\n",
    "            bf16=is_bfloat16_supported(),\n",
    "            logging_steps=1,\n",
    "            optim=\"adamw_8bit\",\n",
    "            weight_decay=0.01,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            seed=3407,\n",
    "            output_dir=\"outputs\",\n",
    "        ),\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetune llama3 to deduce part from utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_fn_instruction = lambda row: f\"What part of a {row.source_object_class} does the next utterance describe? Return a single word using only one of the following options: {', '.join(list(shape_category_to_part_to_key_words[row.source_object_class].keys()))}. If none of the parts is described in the utterance return none.\"\n",
    "part_fn_input = lambda row: row.utterance\n",
    "model, tokenizer = build_model_and_tokenizer()\n",
    "finetune_dataset = build_dataset(\n",
    "    finetune_df,\n",
    "    part_fn_instruction,\n",
    "    part_fn_input,\n",
    "    lambda row: row.part_llama3,\n",
    "    tokenizer.eos_token,\n",
    ")\n",
    "trainer = build_trainer(model, tokenizer, finetune_dataset)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"part_llama3\" not in df.columns:\n",
    "    df[\"part_llama3\"] = \"\"\n",
    "FastLanguageModel.for_inference(model)\n",
    "count = 0\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if row.part_llama3:\n",
    "        continue\n",
    "    inputs = tokenizer(\n",
    "        [\n",
    "            alpaca_prompt.format(\n",
    "                part_fn_instruction(row),\n",
    "                part_fn_input(row),\n",
    "                \"\",\n",
    "            )\n",
    "        ],\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\") \n",
    "    outputs = model.generate(**inputs, max_new_tokens=64, use_cache = True)\n",
    "    output = (\n",
    "        tokenizer.batch_decode(outputs)[0]\n",
    "        .split(\"### Response:\")[1]\n",
    "        .split(\"\\n\")[1]\n",
    "        .split(tokenizer.eos_token)[0]\n",
    "    )\n",
    "    df.at[i, \"part_llama3\"] = output\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        df.to_csv(data_csv_path, index=True, index_label=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_fn_instruction = lambda row: f\"What part of a {row.source_object_class} does the next utterance describe? Return a single word using only one of the following options: {', '.join(list(shape_category_to_part_to_key_words[row.source_object_class].keys()))}.\"\n",
    "part_fn_input = lambda row: row.utterance\n",
    "model, tokenizer = build_model_and_tokenizer()\n",
    "finetune_dataset = build_dataset(\n",
    "    finetune_df[finetune_df.part_llama3 != \"none\"],\n",
    "    part_fn_instruction,\n",
    "    part_fn_input,\n",
    "    lambda row: row.part_llama3,\n",
    "    tokenizer.eos_token,\n",
    ")\n",
    "trainer = build_trainer(model, tokenizer, finetune_dataset)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"part_llama3_no_none\" not in df.columns:\n",
    "    df[\"part_llama3_no_none\"] = \"\"\n",
    "FastLanguageModel.for_inference(model)\n",
    "count = 0\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if row.part_llama3_no_none:\n",
    "        continue\n",
    "    inputs = tokenizer(\n",
    "        [\n",
    "            alpaca_prompt.format(\n",
    "                part_fn_instruction(row),\n",
    "                part_fn_input(row),\n",
    "                \"\",\n",
    "            )\n",
    "        ],\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\") \n",
    "    outputs = model.generate(**inputs, max_new_tokens=64, use_cache = True)\n",
    "    output = (\n",
    "        tokenizer.batch_decode(outputs)[0]\n",
    "        .split(\"### Response:\")[1]\n",
    "        .split(\"\\n\")[1]\n",
    "        .split(tokenizer.eos_token)[0]\n",
    "    )\n",
    "    df.at[i, \"part_llama3_no_none\"] = output\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        df.to_csv(data_csv_path, index=True, index_label=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set part and locality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_csv_path, index_col=\"id\")\n",
    "\n",
    "def set_part(row):\n",
    "    if row.part_keywords != \"none\":\n",
    "        return row.part_keywords\n",
    "    elif row.part_llama3 != \"none\":\n",
    "        return row.part_llama3\n",
    "    else:\n",
    "        return row.part_llama3_no_none\n",
    "    \n",
    "df[\"part\"] = df.apply(lambda row: set_part(row), axis=1)\n",
    "df[\"is_local\"] = df.apply(lambda row: row.part_keywords != \"none\" or row.part_llama3 != \"none\", axis=1)\n",
    "df.to_csv(data_csv_path, index=True, index_label=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetune llama3 to rewrite the comparative utterance as a descriptive utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_fn_instruction = (\n",
    "    lambda row: f\"Rewrite the comparative utterance of a {row.target_random_wnlemma} as a descriptive utterance about a {row.target_random_wnlemma}.\"\n",
    ")\n",
    "utterance_fn_input = lambda row: row.utterance\n",
    "model, tokenizer = build_model_and_tokenizer()\n",
    "finetune_dataset = build_dataset(\n",
    "    finetune_df,\n",
    "    utterance_fn_instruction,\n",
    "    utterance_fn_input,\n",
    "    lambda row: row.utterance_llama3,\n",
    "    tokenizer.eos_token,\n",
    ")\n",
    "trainer = build_trainer(model, tokenizer, finetune_dataset)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"utterance_llama3\" not in df.columns:\n",
    "    df[\"utterance_llama3\"] = \"\"\n",
    "FastLanguageModel.for_inference(model)\n",
    "count = 0\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if row.utterance_llama3:\n",
    "        continue\n",
    "    inputs = tokenizer(\n",
    "        [\n",
    "            alpaca_prompt.format(\n",
    "                utterance_fn_instruction(row),\n",
    "                utterance_fn_input(row),\n",
    "                \"\",\n",
    "            )\n",
    "        ],\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
    "    output = (\n",
    "        tokenizer.batch_decode(outputs)[0]\n",
    "        .split(\"### Response:\")[1]\n",
    "        .split(\"\\n\")[1]\n",
    "        .split(tokenizer.eos_token)[0]\n",
    "    )\n",
    "    df.at[i, \"utterance_llama3\"] = output\n",
    "    print(row.utterance, \"->\", output)\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        df.to_csv(data_csv_path, index=True, index_label=\"id\")\n",
    "df.to_csv(data_csv_path, index=True, index_label=\"id\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
